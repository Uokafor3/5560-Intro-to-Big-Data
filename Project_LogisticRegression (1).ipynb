{"cells":[{"cell_type":"markdown","source":["## CIS5560: PySpark Pipeline Text Sentiment Analysis in Databricks\n\n### by Team 4 (Uche, Raymond, Tofunmi and Sweta) edited on 05/15/2020\nTested in Runtime 6.5 (Spark 2.4.5/2.4.0 Scala 2.11) of Databricks CE"],"metadata":{}},{"cell_type":"markdown","source":["## Text Analysis\nIn this project, we created a classification model that performs sentiment analysis of reviews of different businesses.\n### Import Spark SQL and Spark ML Libraries\n\nFirst, import the libraries you will need:"],"metadata":{}},{"cell_type":"markdown","source":["## Steps to download dataset and do some data engineering (Cleaning up dataset) before importing into databricks\n\nall dataset engineering were done in Jupyter Notebook before importing into databricks\n\ndataset link: https://www.kaggle.com/darshank2019/review#yelp_academic_dataset_review.csv\n\ndownload dataset and using a Jupyter Notebook(we used google colab), we accessed the dataset with total rows = 6685900\n\nwe took a slice of the full dataset of the first 1500000 rows and used that as our full dataset.\n\nwe removed the inverted commas and the letter \"b\" present in all rows (data cleaning)\n\nwe converted the alphanumeric values in the user_id, review_id, & business_id to numeric values\n\nwe tried to drop rows wit missing values and counted the total number of rows again and it was still 1500000.\n\nwe created a subset of our cleaned dataset named df_ml_csv with 120000 rows which we used for both Azure ML & Databricks\n\nNOTE: the .py & .ipynb files containing all codes used for data engineering and analysis is included in the total submission package and is availble in our github link"],"metadata":{}},{"cell_type":"markdown","source":["Import the df_ml.csv dataset"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.linalg import Vectors, SparseVector\nfrom pyspark.ml.clustering import LDA, BisectingKMeans\nfrom pyspark.sql.functions import monotonically_increasing_id\n\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\n\nimport re"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### Load Source Data\nNow load the df_ml data into a DataFrame. This data consists of reviews(column = \"text\") that have been previously captured and classified as positive or negative."],"metadata":{}},{"cell_type":"markdown","source":["Read csv file from DBFS (Databricks File Systems)\n\n## follow the direction to read your table after upload it to Data at the left frame\nNOTE: See above for the data type - \n\nAfter df_ml_csv file is added to the data of the left frame, create a table using the UI, especially, \"Upload File\"\ntick header and infer schema before creating table"],"metadata":{}},{"cell_type":"code","source":["IS_SPARK_SUBMIT_CLI = True\n\nif IS_SPARK_SUBMIT_CLI:\n    sc = SparkContext.getOrCreate()\n    spark = SparkSession(sc)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["%fs ls /FileStore/tables/df_ml.csv"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/df_ml.csv</td><td>df_ml.csv</td><td>77730282</td></tr></tbody></table></div>"]}}],"execution_count":9},{"cell_type":"code","source":["if IS_SPARK_SUBMIT_CLI:\n   df_ml = spark.read.csv('df_ml.csv', inferSchema=True, header=True)\nelse:\n    df_ml = spark.sql(\"SELECT * FROM df_ml_csv\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["df_ml.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------+--------------------+-------------------+---------+-----------+-----+----+------+-----+\n_c0|user_id|                text|               date|review_id|business_id|funny|cool|useful|stars|\n+---+-------+--------------------+-------------------+---------+-----------+-----+----+------+-----+\n  0|  21172|Total bill for th...|2013-05-07 04:34:36|    99203|      14562|    1|   0|     6|  1.0|\n  1|  74272|I *adore* Travis ...|2017-01-14 21:30:33|    72213|      13614|    0|   0|     0|  5.0|\n  2|  67465|I have to say tha...|2016-11-09 20:09:03|    47626|       6862|    0|   0|     3|  5.0|\n  3|  32162|Went in for a lun...|2018-01-09 20:56:38|    39301|      12636|    0|   0|     0|  5.0|\n  4|  32430|Today was my seco...|2018-01-30 23:07:38|    23694|        852|    0|   0|     7|  1.0|\n+---+-------+--------------------+-------------------+---------+-----------+-----+----+------+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["### Prepare the Data\nThe features for the classification model will be derived from the review text. The label is the stars (stars > 2 = 1 (positive review) else: 0 (negative review)"],"metadata":{}},{"cell_type":"code","source":["data = df_ml.select(\"text\", ((col(\"stars\") > 2).cast(\"Double\").alias(\"label\")))\n# data = csv\ndata.show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+-----+\n                text|label|\n+--------------------+-----+\nTotal bill for th...|  0.0|\nI *adore* Travis ...|  1.0|\nI have to say tha...|  1.0|\nWent in for a lun...|  1.0|\nToday was my seco...|  0.0|\n+--------------------+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["### Split the Data\nIn common with most classification modeling processes, we splitted the data into a set for training, and a set for testing the trained model."],"metadata":{}},{"cell_type":"code","source":["splits = data.randomSplit([0.7, 0.3])\ntrain = splits[0]\ntest = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\ntrain_rows = train.count()\ntest_rows = test.count()\nprint(\"Training Rows:\", train_rows, \" Testing Rows:\", test_rows)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Training Rows: 83824  Testing Rows: 36176\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["### Define the Pipeline\nThe pipeline for the model consist of the following stages:\n- A Tokenizer to split the tweets into individual words.\n- A StopWordsRemover to remove common words such as \"a\" or \"the\" that have little predictive value.\n- A HashingTF class to generate numeric vectors from the text values.\n- A LogisticRegression algorithm to train a binary classification model."],"metadata":{}},{"cell_type":"code","source":["# convert sentence to words' list\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"SentimentWords\")\n# remove stop words\nswr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"MeaningfulWords\")\n# convert word to number as word frequency\nhashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n# set the model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10, regParam=0.01)\n\n# process pipeline with the series of transforms - 4 transforms\npipeline = Pipeline(stages=[tokenizer, swr, hashTF, lr])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["### Run the Pipeline as an Estimator\nThe pipeline itself is an estimator, and so it has a **fit** method that we called to run the pipeline on a specified DataFrame. In this case, we ran the pipeline on the training data to train a model."],"metadata":{}},{"cell_type":"code","source":["piplineModel = pipeline.fit(train)\nprint(\"Pipeline complete!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Pipeline complete!\n</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["### Test the Pipeline Model\nThe model produced by the pipeline is a transformer that will apply all of the stages in the pipeline to a specified DataFrame and apply the trained model to generate predictions. In this case, we transformed the **test** DataFrame using the pipeline to generate label predictions."],"metadata":{}},{"cell_type":"code","source":["prediction = piplineModel.transform(test)\npredicted = prediction.select(\"text\", \"prediction\", \"trueLabel\")\npredicted.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+----------+---------+\n                text|prediction|trueLabel|\n+--------------------+----------+---------+\n#1 it should be a...|       1.0|      0.0|\n$1 oysters everyd...|       1.0|      1.0|\n$79 dollars for a...|       1.0|      0.0|\n* 4.5 stars \\n\\nI...|       1.0|      1.0|\n*****************...|       1.0|      1.0|\n*****************...|       0.0|      0.0|\n**I have amended ...|       1.0|      1.0|\n**Maybe a 2 star,...|       0.0|      0.0|\n**UPDATE**\\nWashe...|       0.0|      0.0|\n--LOOK AT MY PICT...|       1.0|      0.0|\n+--------------------+----------+---------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["predicted10 = prediction.select(\"*\")\npredicted10.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n                text|trueLabel|      SentimentWords|     MeaningfulWords|            features|       rawPrediction|         probability|prediction|\n+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n#1 it should be a...|      0.0|[#1, it, should, ...|[#1, advertised, ...|(262144,[14,2433,...|[-2.5650975052305...|[0.07141874592488...|       1.0|\n$1 oysters everyd...|      1.0|[$1, oysters, eve...|[$1, oysters, eve...|(262144,[16422,17...|[-6.6968567796218...|[0.00123326419840...|       1.0|\n$79 dollars for a...|      0.0|[$79, dollars, fo...|[$79, dollars, ai...|(262144,[14868,21...|[-1.2453808220855...|[0.22350076473034...|       1.0|\n* 4.5 stars \\n\\nI...|      1.0|[*, 4.5, stars, \\...|[*, 4.5, stars, \\...|(262144,[14,78,47...|[-17.852244869067...|[1.76550339125282...|       1.0|\n*****************...|      1.0|[****************...|[****************...|(262144,[3008,518...|[-16.773881086590...|[5.19034207525053...|       1.0|\n*****************...|      0.0|[****************...|[****************...|(262144,[19939,21...|[10.7361021928085...|[0.99997825493968...|       0.0|\n**I have amended ...|      1.0|[**i, have, amend...|[**i, amended, re...|(262144,[24113,26...|[-3.6126032785629...|[0.02627263934820...|       1.0|\n**Maybe a 2 star,...|      0.0|[**maybe, a, 2, s...|[**maybe, 2, star...|(262144,[14,3189,...|[5.44736291364238...|[0.99571082688474...|       0.0|\n**UPDATE**\\nWashe...|      0.0|[**update**\\nwash...|[**update**\\nwash...|(262144,[14,2841,...|[17.0731875012253...|[0.99999996152232...|       0.0|\n--LOOK AT MY PICT...|      0.0|[--look, at, my, ...|[--look, picture!...|(262144,[14,23192...|[-0.6135929202065...|[0.35124004054298...|       1.0|\n+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["##TP, FP, TN, and FN all calculated\nPrecision and recall also calculated"],"metadata":{}},{"cell_type":"code","source":["tp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 1\").count())\nfp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 0\").count())\ntn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 0\").count())\nfn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 1\").count())\nmetrics = spark.createDataFrame([\n      (\"TP\", tp),\n      (\"FP\", fp),\n      (\"TN\", tn),\n      (\"FN\", fn),\n      (\"Precision\", tp / (tp + fp)),\n      (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\nmetrics.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+------------------+\n   metric|             value|\n+---------+------------------+\n       TP|           26711.0|\n       FP|            2577.0|\n       TN|            5671.0|\n       FN|            1217.0|\nPrecision|0.9120117454247474|\n   Recall|0.9564236608421656|\n+---------+------------------+\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["evaluator = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\naur = evaluator.evaluate(prediction)\nprint (\"AUR =\", aur)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">AUR = 0.8219921407993562\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["## AUC is calculated"],"metadata":{}},{"cell_type":"code","source":["gbt_evaluator =  MulticlassClassificationEvaluator(labelCol=\"trueLabel\", predictionCol=\"prediction\")\ngbt_auc = gbt_evaluator.evaluate(prediction)\n\nprint(\"AUC for Logistic Regression = \", gbt_auc)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">AUC for Logistic Regression =  0.8916586030414259\n</div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["## Generated AUC for Logistic Regression =  0.8916586030414259"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":31}],"metadata":{"name":"Project_LogisticRegression","notebookId":4252204239593162},"nbformat":4,"nbformat_minor":0}

# -*- coding: utf-8 -*-
"""Yelp_Review_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VlKlVX0haZ2VE5dXEe7yqYXaaDVMwdwl
"""

from google.colab import drive
drive.mount('/content/drive')

"""download dataset from kaggle.com and save in google drive"""

#! wget -O yelp_reviews.zip 'https://storage.googleapis.com/kaggle-data-sets/176267/397518/compressed/yelp_academic_dataset_review.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1588125031&Signature=ptQQ%2F%2BVilh%2FPes3vB%2BODAQ%2BfHmAktz98JZ1ld2sDIS3jkD8GcLxgb0HWeJIYpuUH3%2BUr90J89LUHAiujailyAVa5VCEnZJkkBsB1JTUc3SQpGhEg9ncbGPcU0ptHAuS8BLP5aO4RDgkLu9otDAR6m0ja3BpFzR%2F4VvutOaSf783%2FUX6Ekc7%2BPjW%2Fbn3uxFDF1YUDnzizXZQ6sEyjOFPnxzCSqZ2rOqH8SN1LwzV%2B8uO52mPjDYwoLCWULK28%2FIoWf3G%2BwOkAdLlOA6HEUVPAB3Pv0odkLd4d1ni9rewjbM95fBjnqYZnSr%2FkvTgNhmPnD3VH1ZUMtDWnPkumIJXBfw%3D%3D&response-content-disposition=attachment%3B+filename%3Dyelp_academic_dataset_review.csv.zip'
#! unzip yelp_reviews.zip

#! ls -la

import pandas as pd

"""Read dataset from google drive"""

df = pd.read_csv('/content/drive/My Drive/yelp_academic_dataset_review.csv')

df.head()

"""### Clean dataset by removing the b in front of all entries and also the inverted commas present in all string values"""

for col in df.columns:
    df[col] = df[col].apply(lambda x: x[1:].lstrip(').rstrip('))

for col in df.columns:
    if col not in ['funny','cool','useful','stars']:
        df[col] = df[col].apply(lambda x: x.lstrip("'").rstrip("'").lstrip('"').rstrip('"'))

df_head()



"""Create a subset from of 1500000 rows. This would be used for Oracle BDCE Machine Learning"""

df_ml = df[0:1500000:1]

len(df_ml)

"""Write new dataframe into google drive"""

#df1.to_csv('/content/drive/My Drive/clean_data.csv')

"""Read dataframe fro googledrive"""

#df_ml = pd.read_csv('/content/drive/My Drive/clean_data.csv')

"""Convert Alphanumeric values of string data type into Numeric Values"""

id_list = list(set(df_ml.user_id))
id_list2 = list(set(df_ml.business_id))
id_list3 = list(set(df_ml.review_id))

df_ml['user_id'] = df_ml['user_id'].apply(lambda x: id_list.index(x))

df_ml['business_id'] = df_ml['business_id'].apply(lambda x: id_list2.index(x))

df_ml['review_id'] = df_ml['review_id'].apply(lambda x: id_list3.index(x))

df_ml.head()

"""Normalizing User_id, business_id, and review_id columns in preparation for Machine Learning"""

from sklearn.preprocessing import StandardScaler
from sklearn import preprocessing

x = df_ml[['user_id']].values.astype(float)
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df_normalized = pd.DataFrame(x_scaled)

y = df_ml[['business_id']].values.astype(float)
min_max_scaler = preprocessing.MinMaxScaler()
y_scaled = min_max_scaler.fit_transform(y)
df_norm = pd.DataFrame(y_scaled)

z = df_ml[['review_id']].values.astype(float)
min_max_scaler = preprocessing.MinMaxScaler()
y_scaled = min_max_scaler.fit_transform(z)
df_normal = pd.DataFrame(y_scaled)

df_ml['user_id'] = df_normalized

df_ml['business_id'] = df_norm

df_ml['review_id'] = df_normal

df_ml.head()

"""Write new dataframe into google drive
df_ml will be used for Oracle BDCE while df2 will be used for databricks BDCE
"""

df2 = df_ml[0:120000:1]

df_ml.to_csv('/content/drive/My Drive/scaled_clean_data.csv')

df2.to_csv('/content/drive/My Drive/scaled_subset.csv')

"""DATA ANALYSIS USING SCALED_SUBSET.CSV"""

#IMPORTING DATA FRAME ONCE MORE

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import pandas.util.testing as tm
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
plt.rcParams['figure.figsize'] = 12, 6
import warnings
warnings.filterwarnings('ignore')

df_ml = pd.read_csv('/content/drive/My Drive/scaled_subset.csv')

"""Boxplots and Linear Regression plots to show relatioships between the useful, cool, funny and stars column"""

vis1 = sns.boxplot(data = df_ml, x = 'stars', y = 'cool')

vis2 = sns.boxplot(data = df_ml, x = 'stars', y = 'funny')

vis3 = sns.boxplot(data = df_ml, y = 'useful', x = 'stars')

sns.set_style('darkgrid')
f, axes = plt.subplots(2, 2, figsize = (15, 15))
vis1 = sns.boxplot(data = df_ml, x = 'stars', y = 'cool', ax = axes[0, 0])
vis2 = sns.boxplot(data = df_ml, x = 'stars', y = 'funny', ax = axes[0, 1])
vis3 = sns.boxplot(data = df_ml, y = 'useful', x = 'stars', ax = axes[1, 0])
vis01 = sns.lmplot(data = df_ml, x = 'cool', y = 'cool', fit_reg = False, hue = 'stars', scatter_kws = {'s':100})

vis11 = sns.lmplot(data = df_ml, x = 'cool', y = 'funny', fit_reg = False, hue = 'stars', scatter_kws = {'s':100})

vis12 = sns.lmplot(data = df_ml, x = 'cool', y = 'useful', fit_reg = False, hue = 'stars', scatter_kws = {'s':100})

vis13 = sns.lmplot(data = df_ml, x = 'useful', y = 'funny', fit_reg = False, hue = 'stars', scatter_kws = {'s':100})

"""PLOT to generate Top 25 Words In from the text column for each of the ratings in the stars column"""

import nltk

nltk.download('stopwords')
nltk.download('punkt')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.util import bigrams
import string
import re

stop_words = set(stopwords.words('english'))

"""Create new column 'tokens'"""

df_ml['tokens'] = df_ml['text'].apply(lambda x: word_tokenize(x) )

def rm_stop_words(tokens):
    filtered = [w for w in tokens if not w.lower() in stop_words] 
    return filtered

def rm_puctuation(tokens):
    init_filter = [w for w in tokens if w not in string.punctuation]
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in init_filter]
    clean = [w for w in stripped if w not in ['',' ']]
    return clean

def filter_tokens(tokens):
    filtered = rm_stop_words(tokens)
    new_tokens = rm_puctuation(filtered)
    return new_tokens

"""Create new column 'tokens_no_stopwords'"""

df_ml['tokens_no_stopwords'] = df_ml['tokens'].apply(lambda x: filter_tokens(x))

"""Create new column 'bigrams_'"""

df_ml['bigrams_'] = df_ml.tokens_no_stopwords.apply(lambda x: list(bigrams(x)))

o_reviews = df_ml[df_ml.stars == 1.0]
t_reviews = df_ml[df_ml.stars == 2.0]
th_reviews = df_ml[df_ml.stars == 3.0]
f_reviews = df_ml[df_ml.stars == 4.0]
fi_reviews = df_ml[df_ml.stars == 5.0]
#th_reviews.head()

o_fdist = FreqDist(word.lower() for word_lst in o_reviews.tokens_no_stopwords for word in word_lst)
t_fdist = FreqDist(word.lower() for word_lst in t_reviews.tokens_no_stopwords for word in word_lst)
th_fdist = FreqDist(word.lower() for word_lst in th_reviews.tokens_no_stopwords for word in word_lst)
f_fdist = FreqDist(word.lower() for word_lst in f_reviews.tokens_no_stopwords for word in word_lst)
fi_fdist = FreqDist(word.lower() for word_lst in fi_reviews.tokens_no_stopwords for word in word_lst)

"""PLOT GRAPH FOR TOP 25 WORDS IN EACH OF THE RATINGS IN STARS COLUMN"""

o_fdist.plot(25,title="Top 25 Words In 1 star")

t_fdist.plot(25,title="Top 25 Words In 2 star")

th_fdist.plot(25,title="Top 25 Words In 3 star")

f_fdist.plot(25,title="Top 25 Words In 4 star")

fi_fdist.plot(25,title="Top 25 Words In 5 star")

"""Plot graphs of day, months with stars as legend to understand the relation if any"""

import os
import pandas as pd
import pandas_profiling as pf
from datetime import datetime as dt
import calendar
import pickle

def get_day(date_string):
    return calendar.day_name[dt.strptime(date_string, "%Y-%m-%d %H:%M:%S").weekday()]
def get_month(date_string):
    return calendar.month_name[dt.strptime(date_string, "%Y-%m-%d %H:%M:%S").month]

"""Create new columns day and month"""

df_ml['day'] = df_ml.date.apply(lambda x: get_day(x))
df_ml['month'] = df_ml.date.apply(lambda x: get_month(x))

pd.crosstab(df_ml.day, df_ml.stars).plot(kind='bar',rot=1,title='Distribution of reviews by Day')

pd.crosstab(df_ml.month, df_ml.stars).plot(kind='bar',rot=1,title='Distribution of reviews by Months')